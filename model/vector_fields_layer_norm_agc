import torch
import torch.nn as nn
import torch.nn.functional as F

class FinalTanh_f(nn.Module):
    def __init__(self, input_channels, hidden_channels, hidden_hidden_channels, num_hidden_layers):
        super(FinalTanh_f, self).__init__()
        
        self.input_channels = input_channels
        self.hidden_channels = hidden_channels
        self.hidden_hidden_channels = hidden_hidden_channels
        self.num_hidden_layers = num_hidden_layers

        self.linear_in = nn.Linear(hidden_channels, hidden_hidden_channels)
        
        self.linears = nn.ModuleList(torch.nn.Linear(hidden_hidden_channels, hidden_hidden_channels)
                                           for _ in range(num_hidden_layers - 1))
        self.linear_out = nn.Linear(hidden_hidden_channels, input_channels * hidden_channels) #32,32*4  -> # 32,32,4 

    def extra_repr(self):
        return "input_channels: {}, hidden_channels: {}, hidden_hidden_channels: {}, num_hidden_layers: {}" \
               "".format(self.input_channels, self.hidden_channels, self.hidden_hidden_channels, self.num_hidden_layers)

    def forward(self, z):
        z = self.linear_in(z)
        z = z.relu()

        for linear in self.linears:
            z = linear(z)
            z = z.relu()

        z = self.linear_out(z).view(*z.shape[:-1], self.hidden_channels, self.input_channels)    
        z = z.tanh()
        return z



class VectorField_g(torch.nn.Module):
    def __init__(self, input_channels, hidden_channels, hidden_hidden_channels, num_hidden_layers, num_nodes, cheb_k, embed_dim,
                    g_type, dropout_prob=0.5):
        super(VectorField_g, self).__init__()
        
        self.input_channels = input_channels
        self.hidden_channels = hidden_channels
        self.hidden_hidden_channels = hidden_hidden_channels
        self.num_hidden_layers = num_hidden_layers
        self.dropout_prob = dropout_prob

        self.linear_in = torch.nn.Linear(hidden_channels, hidden_hidden_channels)
  
        self.linear_out = torch.nn.Linear(hidden_hidden_channels, hidden_channels * hidden_channels) #32,32*4  -> # 32,32,4 
        
        self.g_type = g_type
        if self.g_type == 'agc':
            self.node_embeddings = nn.Parameter(torch.randn(num_nodes, embed_dim), requires_grad=True)
            self.cheb_k = cheb_k
            self.weights_pool = nn.Parameter(torch.FloatTensor(embed_dim, cheb_k, hidden_hidden_channels, hidden_hidden_channels))
            self.bias_pool = nn.Parameter(torch.FloatTensor(embed_dim, hidden_hidden_channels))
            self.num_heads = 8


    def extra_repr(self):
        return "input_channels: {}, hidden_channels: {}, hidden_hidden_channels: {}, num_hidden_layers: {}" \
               "".format(self.input_channels, self.hidden_channels, self.hidden_hidden_channels, self.num_hidden_layers)

    def forward(self, z):
        z = self.linear_in(z)
        z = z.relu()

        if self.g_type == 'agc':
            z = self.agc(z)
        else:
            raise ValueError('Check g_type argument')
        
        z = self.linear_out(z).view(*z.shape[:-1], self.hidden_channels, self.hidden_channels)
        z = z.tanh()
        return z #torch.Size([64, 307, 64, 1])

    def agc(self, z):
        """
        Adaptive Graph Convolution
        - Node Adaptive Parameter Learning
        - Data Adaptive Graph Generation
        """
        device = z.device
        
        node_num = self.node_embeddings.shape[0]
        
        
        E = self.node_embeddings
        head_dim = E.size(1) // self.num_heads
        E_heads = E.view(E.size(0), self.num_heads, head_dim).permute(1, 0, 2)
        
        cos_sim = torch.zeros((self.num_heads, node_num, node_num), device=device)
        
        for h in range(self.num_heads):
            E_h = E_heads[h]
            cos_sim[h] = F.softmax(F.leaky_relu(torch.cos(torch.matmul(E_h, E_h.transpose(0, 1)))))
            
        A_multi_head = cos_sim.mean(dim=0)
        
        #A_multi_head = F.softmax(A_multi_head, dim=1)
        supports = F.layer_norm(A_multi_head, A_multi_head.size()).to(device)
        
        A_adaptive = F.dropout(supports, p=self.dropout_prob, training=self.training)

        A_adaptive = torch.sigmoid(A_adaptive)
        
        # laplacian=False
        laplacian=False
        if laplacian == True:
            
            support_set = [supports, -torch.eye(node_num).to(supports.device)]

        else:
            support_set = [torch.eye(node_num).to(device), A_adaptive]
        
        #default cheb_k = 3
        for k in range(2, self.cheb_k):
            support_set.append(torch.matmul(2 * supports, support_set[-1]) - support_set[-2])
            
        supports = torch.stack(support_set, dim=0).to(device)
        
        weights = torch.einsum('nd,dkio->nkio', self.node_embeddings.to(device), self.weights_pool.to(device))  #N, cheb_k, dim_in, dim_out
        
        bias = torch.matmul(self.node_embeddings.to(device), self.bias_pool.to(device))                       #N, dim_out
        
        x_g = torch.einsum("knm,bmc->bknc", supports, z)      #B, cheb_k, N, dim_in
        
        x_g = x_g.permute(0, 2, 1, 3)  # B, N, cheb_k, dim_in
        
        z = torch.einsum('bnki,nkio->bno', x_g, weights) + bias     #b, N, dim_out
        
        return z
